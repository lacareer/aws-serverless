AWSTemplateFormatVersion: '2010-09-09'
Description: AWS Step Functions sample project for Distributed map
Transform: AWS::Serverless-2016-10-31

Parameters:
  DATASET:
    Type: String
    Default: "https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/2a22e604-2f2e-4d7b-85a8-33b38c999234/dataset/noah.zip"

Resources:
  MultiFileResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled
  MultiFileDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled

  StatesHighPrecipitationRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: states.amazonaws.com
            Action: 'sts:AssumeRole'
      Path: '/'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchFullAccess
        - arn:aws:iam::aws:policy/AWSXrayFullAccess
      Policies:
        - PolicyName: ReadDataPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - S3:GetObject
                  - S3:ListBucket
                Resource:
                  - !GetAtt  MultiFileDataBucket.Arn
                  - !Join ["/", [!GetAtt MultiFileDataBucket.Arn, "*"]]
        - PolicyName: WriteResultsPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - S3:PutObject
                Resource: !Join ["/", [!GetAtt MultiFileResultsBucket.Arn, "*"]]
        - PolicyName: StartExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - states:StartExecution
                Resource: !Sub "arn:aws:states:${AWS::Region}:${AWS::AccountId}:stateMachine:*"
        - PolicyName: InvokeLambdaPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource:
                  - !Join [":", [!GetAtt HighPrecipitationFunction.Arn, "*"]]
                  - !GetAtt HighPrecipitationFunction.Arn


  HighPrecipitationFunction:
    Type: AWS::Serverless::Function
    Properties:
      InlineCode: |
        import json
        import os
        import csv
        import io

        from datetime import datetime
        from decimal import Decimal
        from typing import Dict, List

        import boto3


        S3_CLIENT = boto3.client("s3")


        def lambda_handler(event: dict, context):
            """Handler that will find the weather station that has the highest average temperature by month.

            Returns a dictionary with "year-month" as the key and dictionary (weather station info) as value.

            """
            print(event)
            input_bucket_name = event["BatchInput"]["Bucket"]

            high_by_station: Dict[str, float] = {}

            for item in event["Items"]:
                csv_data = get_file_from_s3(input_bucket_name, item["Key"])
                dict_data = get_csv_dict_from_string(csv_data)
                station = None 
                high_prcp = 0
                for row in dict_data:
                    if row['ELEMENT'] == 'PRCP':
                        if not station:
                            station = row["ID"]
                        prcp = float(row["DATA_VALUE"])
                        if prcp > high_prcp:
                            high_prcp = prcp
                high_by_station[station] = high_prcp   
            print(high_by_station) 
            _write_results_to_ddb(high_by_station)



        def _write_results_to_ddb(high_by_station: Dict[str, Dict]):
            dynamodb = boto3.resource("dynamodb")
            table = dynamodb.Table(os.environ["RESULTS_DYNAMODB_TABLE_NAME"])

            for station, prcp in high_by_station.items():
              if station is not None:
                row = {}
                row["pk"] = station
                row["PRCP"] = round(Decimal(prcp))
                table.put_item(Item=row)


        def get_file_from_s3(input_bucket_name: str, key: str) -> str:
            resp = S3_CLIENT.get_object(Bucket=input_bucket_name, Key=key)
            return resp["Body"].read().decode("utf-8")


        def get_csv_dict_from_string(csv_string: str) -> dict:
            return csv.DictReader(io.StringIO(csv_string))

      Handler: index.lambda_handler
      MemorySize: 2048
      Timeout: 600
      Runtime: python3.10
      Environment:
        Variables:
          RESULTS_DYNAMODB_TABLE_NAME: !Ref ResultsDynamoDBTable
      Policies:
        - S3ReadPolicy:
            BucketName: !Ref MultiFileDataBucket
        - DynamoDBWritePolicy:
            TableName: !Ref ResultsDynamoDBTable



  FunctionS3Create:
    Type: 'AWS::Serverless::Function'
    Properties:
      InlineCode: |
          import json
          import boto3
          import zipfile
          import os
          import cfnresponse
          import urllib.request

          def lambda_handler(event, context):
            s3 = boto3.client('s3')
            destination_bucket = os.environ["INPUT_DEST_BUCKET_NAME"]
            source_bucket_url = os.environ["INPUT_SOURCE_BUCKET_URL"]
            extraction_path = '/tmp/extract/'
            extraction_file_location = extraction_path+'dataset/'
            response_status = cfnresponse.SUCCESS
            try:
              # Download the zip file from S3
              
              print(source_bucket_url)
              
              with urllib.request.urlopen(source_bucket_url) as response, open('/tmp/noah.zip', 'wb') as outfile:
                  outfile.write(response.read())
              
              print("File successfully downloaded !")

              # Extract the contents of the zip file
              with zipfile.ZipFile('/tmp/noah.zip', 'r') as zip_ref:
                  zip_ref.extractall(extraction_path)

              # Upload the extracted files to the destination S3 bucket
              for root, dirs, files in os.walk(extraction_path):
                  for file in files:
                      local_path = os.path.join(root, file)
                      s3_key = "csv/by_station/"+file
                      s3.upload_file(local_path, destination_bucket, s3_key)
              response_message = {"Message": "Object copied successfully"}
            except Exception as e:
              response_status = cfnresponse.FAILED
              response_message = {"Error": str(e)}
            finally:
                # Clean up the extraction directory
                for file in os.listdir(extraction_path):
                    file_path = os.path.join(extraction_path, file)
                    os.remove(file_path)
                os.remove('/tmp/noah.zip')
                # os.rmdir(extraction_file_location)
                os.rmdir(extraction_path)
                
                send_response(event, context, response_status, response_message)

          def send_response(event, context, response_status, response_data, reason=None):
            response_body = {
                "Status": response_status,
                "StackId": event['StackId'],
                "RequestId": event['RequestId'],
                "LogicalResourceId": event['LogicalResourceId'],
                "Data": response_data
            }

            if reason:
              response_body['Reason'] = reason

            # Ensure "Data" is always an object
            if not isinstance(response_body['Data'], dict):
              response_body['Data'] = {}

            response_json = json.dumps(response_body)

            cfnresponse.send(event, context, response_status, response_body)
      Handler: index.lambda_handler
      Runtime: python3.10
      Timeout: 900
      Environment:
        Variables:
          INPUT_DEST_BUCKET_NAME: !Ref MultiFileDataBucket
          INPUT_SOURCE_BUCKET_URL: !Ref DATASET
      Policies:
        - S3CrudPolicy:
            BucketName: !Ref MultiFileDataBucket


  CustomResourceS3Create:
    Type: 'Custom::S3Create'
    Properties:
      ServiceToken: !GetAtt FunctionS3Create.Arn

  ResultsDynamoDBTable:
    Type: AWS::Serverless::SimpleTable
    Properties:
      PrimaryKey:
        Name: pk
        Type: String

  # ProcessMultiFileStateMachine:
  #   Type: AWS::StepFunctions::StateMachine
  #   Properties:
  #     RoleArn: !GetAtt [StatesHighPrecipitationRole, Arn]
  #     DefinitionString: !Sub
  #       - |-
  #         {
  #           "Comment": "A description of my state machine",
  #           "StartAt": "Distributed map high precipitation",
  #           "States": {
  #             "Distributed map high precipitation": {
  #               "Type": "Map",
  #               "ItemProcessor": {
  #                 "ProcessorConfig": {
  #                   "Mode": "DISTRIBUTED",
  #                   "ExecutionType": "STANDARD"
  #                 },
  #                 "StartAt": "Lambda Invoke",
  #                 "States": {
  #                   "Lambda Invoke": {
  #                     "Type": "Task",
  #                     "Resource": "arn:${AWS::Partition}:states:::lambda:invoke",
  #                     "OutputPath": "$.Payload",
  #                     "Parameters": {
  #                       "Payload.$": "$",
  #                       "FunctionName": "${FunctionName}"
  #                     },
  #                     "Retry": [
  #                       {
  #                         "ErrorEquals": [
  #                           "Lambda.ServiceException",
  #                           "Lambda.AWSLambdaException",
  #                           "Lambda.SdkClientException",
  #                           "Lambda.TooManyRequestsException"
  #                         ],
  #                         "IntervalSeconds": 2,
  #                         "MaxAttempts": 6,
  #                         "BackoffRate": 2
  #                       }
  #                     ],
  #                     "End": true
  #                   }
  #                 }
  #               },
  #               "Label": "Distributedmaphighprecipitation",
  #               "ResultPath": null,
  #               "ItemBatcher": {
  #                 "MaxItemsPerBatch": 100,
  #                 "BatchInput": {
  #                   "Bucket.$": "$.bucket"
  #                 }                  
  #               },
  #               "MaxConcurrency": 1000,
  #               "End": true,
  #               "ItemReader": {
  #                 "Resource": "arn:aws:states:::s3:listObjectsV2",
  #                 "Parameters": {
  #                   "Bucket": "${Databucket}",
  #                   "Prefix": "csv/by_station"
  #                 }
  #               }
  #             }
  #           }
  #         }
  #       - {
  #           Databucket: !Ref MultiFileDataBucket,
  #           FunctionName: !GetAtt  HighPrecipitationFunction.Arn
  #         }

Outputs:
  DynamoDBTableName:
    Description: DynamoDB table name where final results are written
    Value: !Ref ResultsDynamoDBTable
  # StateMachineArn:
  #   Value:
  #     Ref: ProcessMultiFileStateMachine

